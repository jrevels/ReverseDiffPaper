\documentclass[10pt]{article}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}

\begin{document}
\title{Notes on benchmark models}
\maketitle

\section{MNIST/Neural network}
This discussion is based on the TensorFlow MNIST tutorial at  \url{https://www.tensorflow.org/versions/r0.12/tutorials/mnist/pros/index.html}.

The MNIST dataset is the fruit fly of supervised machine learning. It consists of a set of 60,000 training and 10,000 test images (28x28 pixels) of digits 0 -- 9. TensorFlow's "hello, world" example of a deep neural network is building a classifier for this data set. This neural network thus has $28^2=784$ inputs and 1 output (the predicted digit).

\subsection{Single layer}
Let $x = h_0$ be the vector of pixel inputs to the first layer. The activation of the first layer, $h_1$ is given by
\begin{equation}
    h_1 = W \cdot h_0 + b
\end{equation}
with $W$ a 784x10 matrix of weights and $b$ a 10-vector of biases. The probability of reporting a classification of digit $i$ is then given by
\begin{equation}
    y = p(i|x) = \mathrm{softmax}(h_1) = \frac{e^{h_{1i}}}{\sum_i e^{h_{1i}}}
\end{equation}


The objective of training is to minimize the loss function, which in this case is the cross-entropy between $y$ from the model and $\hat{y}$ from the data, which is a 10-vector with a one-hot representation of the digit in the image (i.e., an image depicting the digit 2 is coded as $\hat{y} = \langle 0, 0, 1, 0, 0, 0, 0, 0, 0, 0\rangle$):
\begin{equation}
    \mathcal{L} = \textrm{Cross-entropy}(y, \hat{y}) = \frac{1}{N}\sum_{n = 1, i = 0, 1}^N \left[\hat{y}_{ni}\log y_{ni} + (1 - \hat{y}_{ni}) \log (1 - y_{ni}) \right]
\end{equation}
where $n$ labels training examples and $i$ indices within the vector of predictions.\footnote{TensorFlow actually defines a more numerically stable version of this cross-entropy at \url{https://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#sigmoid_cross_entropy_with_logits} that uses $h_1$ directly.}


\subsection{Multiple layers}
The case of multiple layers works just as above:
\begin{equation}
    h_l = f(W \cdot h_{l-1} + b) \quad l < L
\end{equation}
with $f$ a sigmoid nonlinearity (e.g., tanh, logistic, etc.) and $L$ the number of hidden layers. For the last layer, we do not squash with $f$, but define
\begin{equation}
    y = \mathrm{softmax}(h_L) = \frac{e^{h_{Li}}}{\sum_i e^{h_{Li}}}
\end{equation}
and the computation of the loss function proceeds just as before.


\subsection{Numerical Stability of softmax and cross entropy}
In Neural Networks, it is natural to think and build a multiclass softmax function as a single layer (or single function, in this case), and it is also convenient to offer such function as a separated layer for the users. But in actual practice, to split softmax and cross entropy into separated functions may introduce more numerical instability to the result.

This is a trick when implementing softmax and cross entropy together in neural networks, and Tensorflow is doing better than many other machine learning libraries so far. But when using stochastic gradient descent (like the one we have in ReverseDiff now), it is also possible to run into boundary value problems if the learning rate is large. For example, a learning rate of 3.3e-4 with batch size 100 will likely produce NaNs within five iterations, since the step size is so big that we are reaching the border of cross\_entropy() very quickly.

There could be two improvements:
\begin{enumerate}
    \item Add an error or warning message to let people know the NaNs are being produced due to an inappropriate learning rate;
    \item Implement other optimizer with adaptive learning rate (like AdamOpimizer etc.) in the future. (This is more like an optimization in the long run, so I wouldn't worry about it in the paper.)
\end{enumerate}

\subsection{Pseudo Code}

The reason to use minibatch in model training is because:
\begin{enumerate}
    \item Reduce the input data to a more manageable size for GPU usage in the long run;
    \item To train one batch at a time, and go through all batches within several iterations would not change the result too much (But not vice versa).
\end{enumerate}

Note that the size of minibatch would affect the learning rate of stochastic gradient descent. If the size of minibatch is changed, the learning rate will have to be changed accordingly. The learning rate that gives the best accuracy with our model can be found via function choose\_params() by plotting the learning rate and corresponding test accuracies.

\begin{algorithm}[ht]
\caption{Training Neural Network for MNIST example}
    \begin{algorithmic}[1]
        \State Input: MNIST data, batch size, learning rate
        \State Output: Trained weights and bias from neural network \\
        \State Initiate weights and bias with batch size
        \Procedure{Iterate} {Training}
            \State Load a batch (images, labels) from training dataset
            \State Compute the loss between trained actual labels and calculated labels
            \State Take gradient w.r.t the weights to minimize the loss function
            \State Gradient descent batch.weights -= rate * âˆ‡batch.weights
            \If{stopping condition is met}
                \State Return trained weights and bias
            \EndIf
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\caption{Training Neural Network for MNIST example}
% \SetKwInOut{Input}{Input}
% \SetKwInOut{Output}{Output}
    \begin{algorithmic}[1]
        \State Input: batch size, trained weights, trained bias
        \State Output: Accuracy of model prediction

        \Procedure{Iterate} {Test}
            \State Load a batch (images, labels) from test dataset
            \State Compute predicted labels y = weights * images + bias
            \State Return the matches between y and the actual labels
        \EndProcedure
    \end{algorithmic}
\end{algorithm}


\section{Variational Bayes linear regression}
\subsection{Problem statement}
This example is based on an implementation from the Edward library, which sits atop TensorFlow and facilitates statistical modeling: \url{https://github.com/blei-lab/edward}.

The mathematical model for Bayesian linear regression is given by
\begin{align}
    \label{pmodel}
    \begin{split}
        y &\sim \mathcal{N}(W \cdot x + b, \sigma^2) \\
        W_i &\sim \mathcal{N}(0, \eta^2) \\
        b_i &\sim \mathcal{N}(0, \eta^2)
    \end{split}
\end{align}
That is, $y$, $W$, and $b$ are all normally distributed, with prior standard deviation $\eta$ on the individual elements of $W$ and $b$ and standard deviation $\sigma$ on the observations. Note also that we have written the observations $y$ as scalars, in which case $W$ is a vector. In the case that $y$ is a $d$-vector and $x$ a $m$-vector, the distribution of $y$ remains normal on each element (i.e., diagonal covariance), while $W$ becomes $d \times m$ and $b$ is a $d$-vector.

In variational inference, we use an approximate posterior for each variable on which we want to do inference ($W$ and $b$). In mean field inference, we also assume that the posteriors over each entry are independent an normal:
\begin{align}
    \label{qmodel}
    \begin{split}
        q(w_{ij}) &= \mathcal{N}(\mu_{ij}, \rho^2_{ij}) \\
        q(b_{i}) &= \mathcal{N}(\nu_{i}, \phi^2_{i})
    \end{split}
\end{align}
The goal, then, is to minimize the Kullback-Leibler divergence between the generative/encoding model $p$ in (\ref{pmodel}) and the posterior/decoding model $q$ in (\ref{qmodel}):
\begin{equation}
    \mathcal{D}(q||p) = \int\! dx\; q(x) \log \frac{q(x)}{p(x)} = -\mathbb{E}_q[\log p] - \mathcal{H}[q]
\end{equation}
with $\mathbb{E}_q$ the expectation with respect to the distribution $q$ and $\mathcal{H}[q]$ the differential entropy of the distribution $q$.\footnote{More conventionally, optimization is phrased in terms of maximizing the negative of this quantity, known as the evidence lower bound (ELBO).} In the case of the entropy, one can use the well-known formula for the entropy of the normal distribution with mean $\mu$ and standard deviation $\sigma$:
\begin{equation}
    \mathcal{H} = \frac{1}{2}\log (2\pi) + \log \sigma + \frac{1}{2}
\end{equation}
and since the full posterior $q$ is simply a product of independent normals, the full $\mathcal{H}$ term becomes
\begin{equation}
    \mathcal{H} = \frac{d(m + 1)}{2}(\log (2\pi) + 1) + \sum_{i=1}^d\sum_{j=1}^m \log \rho_{ij} + \sum_{i=1}^d \log \phi_i
\end{equation}
where again, $W$ has $dm$ entries, and $b$ has $d$ entries.

The other contribution to the KL divergence can also be calculated by noting that $\log p$ factorizes over $W$ and $b$ priors and observations and using standard formulas for the log probability density function of the normal distribution. For the case of normal priors and normal posteriors, this can be computed in closed form, but for more general models, this can be an obnoxious algebraic burden. In those cases, one approximates both terms in the optimization objective by drawing samples $z_* \sim q(z)$ (typically just one) and calculating
\begin{equation}
    -\mathbb{E}_q[\log p] - \mathcal{H} \approx -\log p(z_*) + \log q(z_*)
\end{equation}
and taking the gradient of this stochastic expression with respect to the parameters defining the distribution $q$. In this simpler case, the first term above can be written
\begin{align}
    \log p(z) &= \sum_{n = 1}^N \mathrm{logpdf}(y_n|W_*\cdot x_n + b_*, \sigma^2) \\
    &+ \sum_{i = 1}^d\sum_{j=1}^m \mathrm{logpdf}(W_*|0, \eta^2) \\
    &+ \sum_{i = 1}^d \mathrm{logpdf}(b_*|0, \eta^2) \\
\end{align}
with logpdf the log probability density function of the normal distribution:
\begin{equation}
    \mathrm{logpdf}(y|\mu, \sigma^2) = -\frac{1}{2\sigma^2}(y - \mu)^2 - \frac{1}{2}\log(2\pi \sigma^2)
\end{equation}

\subsection{Optimization}
As stated above, the goal is to minimize the KL divergence, but a couple of additional details arise from the fact that $\sigma$, $\eta$, $\rho$, and $\phi$ are required to be positive definite. To transform to unconstrained coordinates, Edward defines variables defined over the whole real line that are related to these strictly positive standard deviations via the softplus transform:
\begin{equation}
    \mathrm{softplus}(x) = \log (1 + e^x)
\end{equation}
Thus, for example, one defines $\sigma = \mathrm{softplus}(\tilde{\sigma})$ and treats $\tilde{\sigma}$ as the variable to be optimized.

\end{document}
