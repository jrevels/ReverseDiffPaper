# Paper Structure

- Introduction
- Methodology
    - Language Features
        - multiple dispatch/operator-overloading design
        - JIT compilation
        - metaprogramming
    - Forward Mode
        - Dual Numbers
            - effectiveness of JIT compilation (show @code_native)
            - memory/cache efficiency
                - stack-allocated partial derivatives
                - chunk size
                - SIMD
            - genericism (nested duals, complex numbers, intervals)
                - calculate second-order subdifferential of a complex function?
                - Interval-Newton method
            - user-defined derivative API
    - Reverse Mode
        - mixed-mode AD
        - taping utilities (Instructions/TrackedReals/TrackedArrays etc.)
            - non-allocating execution ("converts allocating code to pre-allocated code")
            - sparsity exploitation
            - instruction-local specialization (i.e. instruction types)
            - ReverseDiff's tape is useable as a pure-Julia IR
                - dependency analysis is achievable by checking object_ids
                - scheduled parallel execution
                - interval constraint programming
                - converting Julia code to TensorFlow graphs
            - avoiding `getindex` taping
            - destructive assignment
            - fixed-parameter support
            - "nested" tapes
        - genericism
            - support for "special" array types
            - sparse array support
            - GPU-backed array support
        - functor-based directives (@forward, @skip, etc.)
        - constant elision via Julia's conversion system
        - user-defined derivative API
    - Perturbation Confusion
- Benchmarks
    - Softmax
    - Bayesian Linear Regression
    - Convolutional Neural Net
    - Multi-Layer Perceptron
    - Recurrent Neural Net
    - KL Divergence (Celeste)
    - Compare against:
        - AutoGrad.jl/Knet.jl
        - Theano
        - TensorFlow
        - ADOL-C
        - Torch
        - DiffSharp
- Featured Applications
    - Usage statistics?
    - JuMP
        - Coolest downstream examples?
    - Celeste
        - Constraint Transformations
        - KL Divergence
        - CG optimization (Hessian-vector products)
    - MOOSE
        - SIMD vectorization
- Conclusion

# Work Timeline

- Month 1
    - Work:
        - Celeste Work
        - ReverseDiff SoftMax Benchmark
        - ReverseDiff Bayesian Linear Regression Benchmark
        - All TensorFlow Benchmarks

- Month 2
    - Paper Sections:
        - Intro
        - Methodology: Language Features
    - Work:
        - [Support undifferentiated parameters for pre-recorded API](https://github.com/JuliaDiff/ReverseDiff.jl/issues/36)
        - [Support Base.LinearSlow array types](https://github.com/JuliaDiff/ReverseDiff.jl/issues/29)
        - Convolutional Neural Net Benchmark
        - Multi-Layer Perceptron Benchmark
        - Celeste Work
        - All Theano Benchmarks
        - All AutoGrad.jl/Knet.jl Benchmarks

- Month 3
    - Paper Sections:
        - Methodology: Forward Mode
        - Applications
    - Work:
        - [support for GPU-backed arrays](https://github.com/JuliaDiff/ReverseDiff.jl/issues/44)
        - [Exploiting sparsity in higher-order differentiation computations](https://github.com/JuliaDiff/ReverseDiff.jl/issues/41)
        - Recurrent Neural Net Benchmark
        - Celeste Work
        - All Torch Benchmarks
        - All DiffSharp Benchmarks

- Month 4
    - Paper Sections:
        - Methodology: Reverse Mode
        - Methodology: Perturbation Confusion
    - Work:
        - [Perturbation Confusion](https://github.com/JuliaDiff/ReverseDiff.jl/issues/45)
        - [Make it easy for users to inject derivative definitions](https://github.com/JuliaDiff/ReverseDiff.jl/issues/15)
        - All ADOL-C Benchmarks

- Month 5
    - Paper Sections:
        - Revisions
        - Benchmarks
    - Work:
        - "Definitive" benchmark runs on common hardware + performance data analysis
        - Paper Writing
